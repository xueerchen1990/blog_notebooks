{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3447de51-56b5-4560-82d3-279dc5f7434e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miaostar/miniforge3/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fea3927a-d75f-46d2-a60b-59e4671b1702",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 6 files: 100%|████████████████████████| 6/6 [00:00<00:00, 76959.71it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 626 ms, sys: 1.38 s, total: 2.01 s\n",
      "Wall time: 3.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model, tokenizer = load(\"mlx-community/Meta-Llama-3-8B-Instruct-4bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97282e6-8f3a-49be-abf8-0539173c4dd2",
   "metadata": {},
   "source": [
    "### The default example of code generation\n",
    "[link](https://huggingface.co/mlx-community/Meta-Llama-3-8B-Instruct-4bit#use-with-mlx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef4b8e9d-cdb8-4ccc-9a64-62ad4dc485a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: You are a friendly chatbot. hello, what's your name?\n",
      "\")\n",
      "\n",
      "    # Check if the user's message is in the list of expected messages\n",
      "    if message in expected_messages:\n",
      "        # If the user's message is in the list of expected messages, respond with a friendly message\n",
      "        response = \"Hello! My name is Chatbot. I'm here to help you with any questions or concerns you may have. How can I assist you today?\"\n",
      "    else:\n",
      "        # If the user's message is not in the list of expected messages, respond with\n",
      "==========\n",
      "Prompt: 49.385 tokens-per-sec\n",
      "Generation: 19.349 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model, tokenizer, prompt=\"You are a friendly chatbot. hello, what's your name?\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9795e129-d880-4560-bd09-70a3c0731c23",
   "metadata": {},
   "source": [
    "The response is more like a continuation of some pseudo code. Not what we expected. To fix it, let's apply the [chat template](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct#transformers-automodelforcausallm)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27cbe36-176f-44c3-860b-ae8764a59ce9",
   "metadata": {},
   "source": [
    "### Apply the chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c8ff29e-3d1e-46c8-a6e2-2e36dafa2731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000,\n",
       " 128006,\n",
       " 9125,\n",
       " 128007,\n",
       " 271,\n",
       " 2675,\n",
       " 527,\n",
       " 264,\n",
       " 11919,\n",
       " 6369,\n",
       " 6465,\n",
       " 13,\n",
       " 128009,\n",
       " 128006,\n",
       " 882,\n",
       " 128007,\n",
       " 271,\n",
       " 15339,\n",
       " 11,\n",
       " 1148,\n",
       " 596,\n",
       " 701,\n",
       " 836,\n",
       " 30,\n",
       " 128009,\n",
       " 128006,\n",
       " 78191,\n",
       " 128007,\n",
       " 271]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a friendly chatbot.\"},\n",
    "    {\"role\": \"user\", \"content\": \"hello, what's your name?\"},\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd23aaa1-b127-4aa1-b6ac-b10e1d30fb55",
   "metadata": {},
   "source": [
    "input_ids is a list of integers but the `generate` function expects input prompt to be string.\n",
    "\n",
    "So we convert it back to a string by `tokenizer.decode`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ae0e2f2-9c95-49de-ae9f-efe4af4e1f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a friendly chatbot.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nhello, what's your name?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = tokenizer.decode(input_ids)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d54095-1541-4717-a7e4-beb8217154fe",
   "metadata": {},
   "source": [
    "In this new prompt, we can clearly see templates applied to our original question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c947cc6-1dbb-48d9-a0e4-d246de0b08cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a friendly chatbot.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "hello, what's your name?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Hello! My name is Chatty, and I'm a friendly chatbot. I'm here to help answer your questions, provide information, and have a nice conversation with you. How are you today?\n",
      "==========\n",
      "Prompt: 115.986 tokens-per-sec\n",
      "Generation: 19.447 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b90136-0fe4-4131-8d88-55fc7abfb770",
   "metadata": {},
   "source": [
    "Now the response looks much better!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
