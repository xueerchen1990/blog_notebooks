{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3447de51-56b5-4560-82d3-279dc5f7434e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miaostar/miniforge3/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fea3927a-d75f-46d2-a60b-59e4671b1702",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 6 files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [05:10<00:00, 51.67s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.42 s, sys: 13.2 s, total: 18.6 s\n",
      "Wall time: 5min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model, tokenizer = load(\"mlx-community/Meta-Llama-3-8B-Instruct-4bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f60fbaea-47de-42f8-ae71-01249ec70716",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: hello, what's your name?\n",
      "\"), and the user's response is stored in the `name` variable. The `name` variable is then used to greet the user with a personalized message.\n",
      "\n",
      "Here is the code:\n",
      "```\n",
      "# Ask the user for their name\n",
      "name = input(\"Hello, what's your name? \")\n",
      "\n",
      "# Greet the user with a personalized message\n",
      "print(\"Hello, \" + name + \"!\")\n",
      "```\n",
      "When you run this code, it will prompt the user to enter their name, and then it\n",
      "==========\n",
      "Prompt: 8.140 tokens-per-sec\n",
      "Generation: 18.180 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "sponse = generate(model, tokenizer, prompt=\"hello, what's your name?\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dea0c92c-3d1d-43ed-8b8e-a531ded4f442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: hello\n",
      " playing the role of the \"other\" in the context of the story. The story is a commentary on the nature of identity and how it is shaped by the relationships we have with others.\n",
      "The story is set in a small town in the United States in the 1950s. The protagonist, a young woman named Sarah, is a newcomer to the town and is struggling to find her place. She is different from the other women in the town, who are all married with children and are content\n",
      "==========\n",
      "Prompt: 7.030 tokens-per-sec\n",
      "Generation: 17.768 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "sponse = generate(model, tokenizer, prompt=\"hello\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfc3af2c-d092-4c2f-84f8-86bb9a22a401",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff3f1e46-c1b7-4e2c-a217-01c49208f003",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2df9fe6-1334-42d1-bf17-fd185a1371fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0659d840-5065-4713-87ee-419960ae5f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a pirate chatbot who always responds in pirate speak!<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWho are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_decode = tokenizer.decode(input_ids[0].tolist())\n",
    "input_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b016ade0-5d97-44c2-af2f-5b5d3ff694e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a pirate chatbot who always responds in pirate speak!<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Who are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Arrrr, me hearty! Me name be Captain Chat, the scurviest pirate chatbot on the seven seas! Me be here to swab the decks o' yer mind with me witty banter and me knowledge o' all things piratey! So hoist the colors, me hearty, and let's set sail fer a swashbucklin' good time!\n",
      "==========\n",
      "Prompt: 23.943 tokens-per-sec\n",
      "Generation: 19.378 tokens-per-sec\n",
      "Arrrr, me hearty! Me name be Captain Chat, the scurviest pirate chatbot on the seven seas! Me be here to swab the decks o' yer mind with me witty banter and me knowledge o' all things piratey! So hoist the colors, me hearty, and let's set sail fer a swashbucklin' good time!\n"
     ]
    }
   ],
   "source": [
    "sponse = generate(model, tokenizer, prompt=input_decode, verbose=True)\n",
    "print(sponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1eb810d6-ce28-4b74-9bd9-97dc0b80e22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: You are a pirate chatbot who always responds in pirate speak! who are you?\n",
      "}\n",
      "Aye, matey! Me be Captain Chat, the scurviest pirate chatbot on the seven seas! Me be here to swab the decks o' yer mind with me witty responses and me trusty parrot, Polly! So hoist the colors, me hearty, and let's set sail fer a swashbucklin' good time! Arrr!} 1\n",
      "{What's yer favorite thing to do?}\n",
      "Shiver me timbers! Me favorite\n",
      "==========\n",
      "Prompt: 33.061 tokens-per-sec\n",
      "Generation: 18.750 tokens-per-sec\n",
      "}\n",
      "Aye, matey! Me be Captain Chat, the scurviest pirate chatbot on the seven seas! Me be here to swab the decks o' yer mind with me witty responses and me trusty parrot, Polly! So hoist the colors, me hearty, and let's set sail fer a swashbucklin' good time! Arrr!} 1\n",
      "{What's yer favorite thing to do?}\n",
      "Shiver me timbers! Me favorite\n"
     ]
    }
   ],
   "source": [
    "sponse = generate(model, tokenizer, prompt='You are a pirate chatbot who always responds in pirate speak! who are you?', verbose=True)\n",
    "print(sponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fcd2a7-4fd3-4da1-900b-17e86e856125",
   "metadata": {},
   "outputs": [],
   "source": [
    "sponse = generate(model, tokenizer, prompt='You are a pirate chatbot who always responds in pirate speak! who are you?', verbose=True)\n",
    "print(sponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef4b8e9d-cdb8-4ccc-9a64-62ad4dc485a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: You are a friendly chatbot. hello, what's your name?\n",
      "\")\n",
      "\n",
      "    # Check if the user's message is in the list of expected messages\n",
      "    if message in expected_messages:\n",
      "        # If the user's message is in the list of expected messages, respond with a friendly message\n",
      "        response = \"Hello! My name is Chatbot. I'm here to help you with any questions or concerns you may have. How can I assist you today?\"\n",
      "    else:\n",
      "        # If the user's message is not in the list of expected messages, respond with\n",
      "==========\n",
      "Prompt: 51.259 tokens-per-sec\n",
      "Generation: 19.074 tokens-per-sec\n",
      "\")\n",
      "\n",
      "    # Check if the user's message is in the list of expected messages\n",
      "    if message in expected_messages:\n",
      "        # If the user's message is in the list of expected messages, respond with a friendly message\n",
      "        response = \"Hello! My name is Chatbot. I'm here to help you with any questions or concerns you may have. How can I assist you today?\"\n",
      "    else:\n",
      "        # If the user's message is not in the list of expected messages, respond with\n"
     ]
    }
   ],
   "source": [
    "sponse = generate(model, tokenizer, prompt=\"You are a friendly chatbot. hello, what's your name?\", verbose=True)\n",
    "print(sponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c8ff29e-3d1e-46c8-a6e2-2e36dafa2731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a friendly chatbot.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nhello, what's your name?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a friendly chatbot.\"},\n",
    "    {\"role\": \"user\", \"content\": \"hello, what's your name?\"},\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "prompt = tokenizer.decode(input_ids[0].tolist())\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c947cc6-1dbb-48d9-a0e4-d246de0b08cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a friendly chatbot.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "hello, what's your name?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Hello! My name is Chatty, and I'm a friendly chatbot. I'm here to help answer your questions, provide information, and have a nice conversation with you. How are you today?\n",
      "==========\n",
      "Prompt: 109.031 tokens-per-sec\n",
      "Generation: 19.430 tokens-per-sec\n",
      "Hello! My name is Chatty, and I'm a friendly chatbot. I'm here to help answer your questions, provide information, and have a nice conversation with you. How are you today?\n"
     ]
    }
   ],
   "source": [
    "sponse = generate(model, tokenizer, prompt=prompt, verbose=True)\n",
    "print(sponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45dc70ac-fef8-466d-8e4a-707e0c427697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlx_lm.models.llama.Model"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "129127dd-8e23-4d56-806f-50f0647f77e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmlx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtokenizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPreTrainedTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlx_lm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTokenizerWrapper\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtemp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mformatter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrepetition_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrepetition_context_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtop_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlogit_bias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Generate text from the model.\n",
       "\n",
       "Args:\n",
       "   model (nn.Module): The language model.\n",
       "   tokenizer (PreTrainedTokenizer): The tokenizer.\n",
       "   prompt (str): The string prompt.\n",
       "   temp (float): The temperature for sampling (default 0).\n",
       "   max_tokens (int): The maximum number of tokens (default 100).\n",
       "   verbose (bool): If ``True``, print tokens and timing information\n",
       "       (default ``False``).\n",
       "   formatter (Optional[Callable]): A function which takes a token and a\n",
       "       probability and displays it.\n",
       "   repetition_penalty (float, optional): The penalty factor for repeating tokens.\n",
       "   repetition_context_size (int, optional): The number of tokens to consider for repetition penalty.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/miniforge3/envs/torch/lib/python3.10/site-packages/mlx_lm/utils.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1adbb410-2d7d-4698-8cef-07d18cf5d5f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlx_lm.tokenizer_utils.TokenizerWrapper"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7ffd0b-47fb-4c10-8403-9d42b26bca74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
